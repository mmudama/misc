
# Playing with Pipelines

This project explores common data technologies by creating an example pipeline using kafka, flink, avro, parquet, etc. It uses docker containers to separate concerns. The "stream" of data for the kafka topic is generated by polling `/proc/stat` within the `producer` container.

It's also been an experiment to see how much development and integration work I can offload to Copilot. The answer is, an alarming amount.


## Pipeline Architecture

```mermaid
graph LR
    producer["Producer<br/> (polls /proc/stat)"]
    kafka["Kafka Broker<br/>(Port 9092)"]
    registry["Schema Registry<br/>(Port 8081)"]
    
    producer -->|Registers Schema| registry
    producer -->|Publishes Avro| kafka
    
    kafka -->|procstat_snapshots<br/>Confluent Avro| enrichment["CPU Enrichment<br/>Consumer<br/>(Flink Job 1)"]
    enrichment -->|Calculates Deltas<br/>Maintains State| kafka
    kafka -->|procstat_metrics<br/>JSON| parquet["Parquet Writer<br/>Consumer<br/>(Flink Job 2)"]
    
    parquet -->|Writes Columnar| output["./output/<br/>(Parquet Files)"]
    
```

## Assumptions
While docker commands can be run on any platform, the Makefile and instructions assume that you are running commands in a Posix-compatible (Unix) shell. It's been tested on WSL Ubuntu on Windows. Makefiles in particular do not get along with PowerShell or CMD.

## Make it go

This needs to be cleaned up, but for now - 

### Complete rebuild and smoke test (clean slate)
```bash
# 1. Tear down everything
# Warning - this removes Kafka topics and their contents. If you don't want to do this, omit -v
docker compose down -v

# 2. Clean build artifacts
make clean

# 3. Rebuild Docker images and Java consumer
make all

# 4. Start all services
docker compose up -d

# 5. Wait for services to be healthy (~30 seconds)
docker compose ps

# 6. Create Kafka topics and register schema
# These will probably be created automatically by the producer or the flink consumer if you wait a few minutes, but this ensures that they're present.
make setup

# 7. Wait for Flink job submission (~30 seconds)
docker logs jobmanager | grep "submitted"

# 8. Verify producer is writing
docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic procstat_snapshots --from-beginning --max-messages 1 | grep Processed

# 9. Verify enrichment consumer is writing metrics
docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic procstat_metrics --from-beginning --max-messages 1 | grep Processed

# Expected output: JSON with timestamp, time_delta_ms, and cpu_metrics array with percentages
```

## Useful Stuff
```bash
docker compose down # leaves volumes in place
docker compose up -d
docker ps

#use venv for python because ubuntu is snotty about pip
source ~/venv/bin/activate

#list kafka topics
kafka-topics.sh --bootstrap-server localhost:9092 --list

```

* Flink dashboard (in browser): http://localhost:8082


- **Complete setup (first time):**
```bash
make all              # Build Docker images and Java consumer
docker compose up -d  # Start all containers
make setup            # Create Kafka topic and register schema
```

- **Daily workflow:**
```bash
docker compose down
docker compose up -d
```

- **Rebuild Java consumer after code changes:**
```bash
make java-consumer    # or: cd flink/java-consumer && mvn clean package && cp target/procstat-flink-consumer-0.1.0.jar ../
docker compose restart jobmanager taskmanager
```

- **View logs:**
```bash
docker logs producer
docker logs jobmanager
docker logs taskmanager
```

- **Check Flink dashboard:** http://localhost:8082