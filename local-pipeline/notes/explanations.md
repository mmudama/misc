# Explanations for why things are the way they are

- **Why config is baked into the Dockerfile:** We tried using `FLINK_PROPERTIES` environment variable with YAML multi-line strings (`|` and `>-` notation) but Flink's startup scripts don't parse them correctly - newlines get stripped or properties get concatenated. After multiple attempts (pipe notation, fold notation, environment variable arrays), we settled on writing configuration directly to `flink-conf.yaml` in the Dockerfile. This requires image rebuild for config changes but guarantees correct parsing.
- **Why memory settings are required:** Flink 1.19.3 requires explicit memory configuration for both JobManager (`jobmanager.memory.process.size`) and TaskManager (`taskmanager.memory.process.size`). Without these, the containers fail to start with cryptic JVM parameter errors. We use 1600m for JobManager and 1728m for TaskManager.
- **Why entrypoint waits 20 seconds:** The auto-submit script waits for JobManager REST API to be ready, then waits an additional 20 seconds for TaskManager to register its task slots. Without this delay, job submission fails with "no available slots" because the TaskManager hasn't connected yet. This is simpler than polling for slot availability.
- **Why producer is containerized:** The original producer required pip installing dependencies on every container start. Baking dependencies into a custom image (`local/procstat-producer:latest`) makes startup faster and more reliable. The producer also handles schema registration automatically, eliminating manual curl commands.
- **Why a custom Docker image for Flink:** The official Flink images require manual job submission via CLI or dashboard. Our custom image (`local/flink-kafka:1.19.3`) extends the base Flink image with: (1) Kafka connector JARs pre-downloaded at build time, (2) baked configuration in `flink-conf.yaml`, and (3) a custom `entrypoint.sh` that automatically submits the consumer job when JobManager starts. The entrypoint waits for the REST API to be ready, sleeps 20 seconds for TaskManager slot registration, then submits the JAR from `/opt/flink/usrlib/`. This eliminates manual steps and ensures the pipeline starts fully automatically with `docker compose up`. Without this approach, you'd need to manually run `flink run` commands every time you restart the cluster.
